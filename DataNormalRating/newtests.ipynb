{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK\n",
    "def mainCompanySeoFileProcessing():\n",
    "\n",
    "    main_company_file = 'C:\\\\Users\\\\User\\\\OneDrive - Fakulteti i Teknologjise se Informacionit\\\\Desktop\\\\Digitalized\\\\DigiScore\\\\test1.csv'\n",
    "\n",
    "    df=pd.read_csv(main_company_file)\n",
    "\n",
    "    # Create a boolean mask to identify rows to delete\n",
    "    #mask = df['Website Data Type'].isin(['Website Panel', 'Competitors Panel', 'Marketing Channel Distribution Panel', 'Keywords Panel','Category Name','First Competitor','Second Competitor','Third Competitor','Fourth Competitor','Fifth Competitor'])\n",
    "\n",
    "    # Use the boolean mask to select the rows and columns you want to modify\n",
    "    #df.loc[mask, 'Website Data Type':'Website Data'] = None\n",
    "\n",
    "    web_rows_to_delete = ['Website Panel', 'Company Name', 'Competitors Panel', 'Marketing Channel Distribution Panel', 'Keywords Panel','Category Name','First Competitor','Second Competitor','Third Competitor','Fourth Competitor','Fifth Competitor']\n",
    "    # Delete the rows\n",
    "    df.loc[df['Website Data Type'].isin(web_rows_to_delete), 'Website Data Type':'Website Data'] = np.nan\n",
    "\n",
    "    #df['Website Data'] = df['Website Data'].apply(lambda x: float(x.replace('%', '')) / 100 if '%' in str(x) else x)\n",
    "\n",
    "    # Function to process strings\n",
    "    def process_string(x):\n",
    "        str_x = str(x)\n",
    "        # Check if x is a percentage\n",
    "        if '%' in str_x:\n",
    "            return float(str_x.replace('%', '')) / 100\n",
    "        # Check if x starts with a number and is in 'K' or 'M' format\n",
    "        elif str_x[0].isdigit():\n",
    "            if 'K' in str_x:\n",
    "                return float(str_x.replace('K', '')) * 1_000\n",
    "            elif 'M' in str_x:\n",
    "                return float(str_x.replace('M', '')) * 1_000_000\n",
    "            elif ':' in str_x:  # Check if x is in 'HH:MM:SS' or 'MM:SS' format\n",
    "                time_parts = list(map(int, str_x.split(':')))\n",
    "                if len(time_parts) == 3:  # 'HH:MM:SS' format\n",
    "                    hours, minutes, seconds = time_parts\n",
    "                    return float(hours * 3600 + minutes * 60 + seconds)\n",
    "                elif len(time_parts) == 2:  # 'MM:SS' format\n",
    "                    minutes, seconds = time_parts\n",
    "                    return float(minutes * 60 + seconds)\n",
    "            else:  # Convert other numeric strings to float\n",
    "                return float(str_x)\n",
    "        # If x is neither a percentage nor in 'K' or 'M' format or a time, leave it as it is\n",
    "        return x\n",
    "\n",
    "    # Apply the function to each value in the 'Website Data' column\n",
    "    df['Website Data'] = df['Website Data'].apply(process_string)\n",
    "\n",
    "    # Function to convert numbers to float\n",
    "    def convert_to_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return x\n",
    "\n",
    "    # Apply the function to each value in the 'SemRush API Data' column\n",
    "    #df['SemRush API Data'] = df['SemRush API Data'].apply(convert_to_float)\n",
    "\n",
    "    # Identify the rows you want to delete\n",
    "    rows_to_delete = ['Database', 'Domain', 'Date']  # Replace these with the names of the rows you want to delete\n",
    "\n",
    "    # Delete the rows\n",
    "    df.loc[df['SemRush API Data Type'].isin(rows_to_delete), 'SemRush API Data Type':'SemRush API Data'] = np.nan\n",
    "    # Split the DataFrame into two\n",
    "    mainCompanyWebDataDF = df[['Website Data Type', 'Website Data']].copy()\n",
    "    mainCompanySemRushDF = df[['SemRush API Data Type', 'SemRush API Data']].copy()\n",
    "    # Drop rows where all values are missing\n",
    "    mainCompanyWebDataDF.dropna(how='all', inplace=True)\n",
    "    mainCompanySemRushDF.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Append the second DataFrame to the first\n",
    "    #df_final = df1.append(df2, ignore_index=True)\n",
    "\n",
    "    df.to_csv('normalMainCompany.csv',index=False)\n",
    "    mainCompanyWebDataDF.to_csv('WebDataMainCompany.csv',index=False)\n",
    "    mainCompanySemRushDF.to_csv('semRushDataMainCompany.csv',index=False)\n",
    "    #print(df)\n",
    "\n",
    "    # Rename the columns in the second DataFrame\n",
    "    mainCompanySemRushDF.columns = ['Website Data Type', 'Website Data']\n",
    "\n",
    "\n",
    "    # Append the second DataFrame to the first\n",
    "\n",
    "    df_final = pd.concat([mainCompanyWebDataDF, mainCompanySemRushDF], ignore_index=True)\n",
    "\n",
    "    # Sort the DataFrame based on the 'Website Data Type' column\n",
    "    df_final = df_final.sort_values('Website Data Type')\n",
    "    df_final.to_csv('allDataMainCompany.csv',index=False)\n",
    "\n",
    "    #with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        #print(df)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK\n",
    "def competitorsSeoFileProcessing():\n",
    "\n",
    "    competitor_files = ['C:\\\\Users\\\\User\\\\OneDrive - Fakulteti i Teknologjise se Informacionit\\\\Desktop\\\\Digitalized\\\\DigiScore\\\\DataML\\\\Competitors\\\\seoCompetitor_1.csv',\n",
    "                        'C:\\\\Users\\\\User\\\\OneDrive - Fakulteti i Teknologjise se Informacionit\\\\Desktop\\\\Digitalized\\\\DigiScore\\\\DataML\\\\Competitors\\\\seoCompetitor_2.csv',\n",
    "                        'C:\\\\Users\\\\User\\\\OneDrive - Fakulteti i Teknologjise se Informacionit\\\\Desktop\\\\Digitalized\\\\DigiScore\\\\DataML\\\\Competitors\\\\seoCompetitor_3.csv',\n",
    "                        'C:\\\\Users\\\\User\\\\OneDrive - Fakulteti i Teknologjise se Informacionit\\\\Desktop\\\\Digitalized\\\\DigiScore\\\\DataML\\\\Competitors\\\\seoCompetitor_4.csv',\n",
    "                        'C:\\\\Users\\\\User\\\\OneDrive - Fakulteti i Teknologjise se Informacionit\\\\Desktop\\\\Digitalized\\\\DigiScore\\\\DataML\\\\Competitors\\\\seoCompetitor_5.csv']\n",
    "\n",
    "    # Function to process strings\n",
    "    def process_string(x):\n",
    "        str_x = str(x)\n",
    "        # Check if x is a percentage\n",
    "        if '%' in str_x:\n",
    "            return float(str_x.replace('%', '')) / 100\n",
    "        # Check if x starts with a number and is in 'K' or 'M' format\n",
    "        elif str_x[0].isdigit():\n",
    "            if 'K' in str_x:\n",
    "                return float(str_x.replace('K', '')) * 1_000\n",
    "            elif 'M' in str_x:\n",
    "                return float(str_x.replace('M', '')) * 1_000_000\n",
    "            elif ':' in str_x:  # Check if x is in 'HH:MM:SS' or 'MM:SS' format\n",
    "                time_parts = list(map(int, str_x.split(':')))\n",
    "                if len(time_parts) == 3:  # 'HH:MM:SS' format\n",
    "                    hours, minutes, seconds = time_parts\n",
    "                    return float(hours * 3600 + minutes * 60 + seconds)\n",
    "                elif len(time_parts) == 2:  # 'MM:SS' format\n",
    "                    minutes, seconds = time_parts\n",
    "                    return float(minutes * 60 + seconds)\n",
    "            else:  # Convert other numeric strings to float\n",
    "                return float(str_x)\n",
    "        # If x is neither a percentage nor in 'K' or 'M' format or a time, leave it as it is\n",
    "        return x\n",
    "\n",
    "    # Function to convert numbers to float\n",
    "    def convert_to_float(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            return x\n",
    "\n",
    "\n",
    "    # Create an empty list to store the final DataFrames\n",
    "    final_dfs = []\n",
    "    for i, file in enumerate(competitor_files, 1):  # 'i' starts from 1\n",
    "        df = pd.read_csv(file)\n",
    "\n",
    "        # Create a boolean mask to identify rows to delete\n",
    "        #mask = df['Website Data Type'].isin(['Website Panel', 'Competitors Panel', 'Marketing Channel Distribution Panel', 'Keywords Panel','Category Name','First Competitor','Second Competitor','Third Competitor','Fourth Competitor','Fifth Competitor'])\n",
    "\n",
    "        # Use the boolean mask to select the rows and columns you want to modify\n",
    "        #df.loc[mask, 'Website Data Type':'Website Data'] = None\n",
    "\n",
    "        web_rows_to_delete = ['Website Panel','Category Name', 'Company Name',\n",
    "                            'Keywords Panel: Competitor_1','Marketing Channel Distribution Panel: Competitor_1',\n",
    "                            'Keywords Panel: Competitor_2','Marketing Channel Distribution Panel: Competitor_2',\n",
    "                            'Keywords Panel: Competitor_3','Marketing Channel Distribution Panel: Competitor_3',\n",
    "                            'Keywords Panel: Competitor_4','Marketing Channel Distribution Panel: Competitor_4',\n",
    "                            'Keywords Panel: Competitor_5','Marketing Channel Distribution Panel: Competitor_5']\n",
    "        # Delete the rows\n",
    "        df.loc[df['Website Data Type'].isin(web_rows_to_delete), 'Website Data Type':'Website Data'] = np.nan\n",
    "\n",
    "        #df['Website Data'] = df['Website Data'].apply(lambda x: float(x.replace('%', '')) / 100 if '%' in str(x) else x)\n",
    "\n",
    "\n",
    "        # Apply the function to each value in the 'Website Data' column\n",
    "        df['Website Data'] = df['Website Data'].apply(process_string)\n",
    "\n",
    "        # Apply the function to each value in the 'SemRush API Data' column\n",
    "        df[f'SemRush API Data: Competitor_{i}'] = df[f'SemRush API Data: Competitor_{i}'].apply(convert_to_float)\n",
    "\n",
    "        # Identify the rows you want to delete\n",
    "        rows_to_delete = ['Database', 'Domain', 'Date']  # Replace these with the names of the rows you want to delete\n",
    "\n",
    "        # Delete the rows\n",
    "        df.loc[df[f'SemRush API Data Type: Competitor_{i}'].isin(rows_to_delete), f'SemRush API Data Type: Competitor_{i}':f'SemRush API Data: Competitor_{i}'] = np.nan\n",
    "        # Split the DataFrame into two\n",
    "        competitorWebDataDF = df[['Website Data Type', 'Website Data']].copy()\n",
    "        competitorSemRushDF = df[[f'SemRush API Data Type: Competitor_{i}', f'SemRush API Data: Competitor_{i}']].copy()\n",
    "        # Drop rows where all values are missing\n",
    "        competitorWebDataDF.dropna(how='all', inplace=True)\n",
    "        competitorSemRushDF.dropna(how='all', inplace=True)\n",
    "\n",
    "        # Append the second DataFrame to the first\n",
    "        #df_final = df1.append(df2, ignore_index=True)\n",
    "\n",
    "        df.to_csv(f'normalCompetitor{i}.csv',index=False)\n",
    "        competitorWebDataDF.to_csv(f'webDataCompetitor_{i}.csv',index=False)\n",
    "        competitorSemRushDF.to_csv(f'semRushDataCompetitor_{i}.csv',index=False)\n",
    "        #print(df)\n",
    "\n",
    "        # Rename the columns in the second DataFrame\n",
    "        competitorSemRushDF.columns = ['Website Data Type', 'Website Data']\n",
    "\n",
    "\n",
    "        # Append the second DataFrame to the first\n",
    "\n",
    "        df_final = pd.concat([competitorWebDataDF, competitorSemRushDF], ignore_index=True)\n",
    "\n",
    "        # Sort the DataFrame based on the 'Website Data Type' column\n",
    "        df_final = df_final.sort_values('Website Data Type')\n",
    "        df_final.to_csv(f'allDataCompetitor_{i}.csv',index=False)\n",
    "\n",
    "        # Append the final DataFrame for this iteration to the list\n",
    "        final_dfs.append(df_final)\n",
    "\n",
    "        #with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "            #print(df)\n",
    "    # Return the list of final DataFrames\n",
    "    return final_dfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for each source\n",
    "df_main = pd.read_csv('regularData\\\\main.csv')\n",
    "df_comp1 = pd.read_csv('regularData\\\\comp1.csv')\n",
    "df_comp2 = pd.read_csv('regularData\\\\comp1.csv')\n",
    "df_comp3 = pd.read_csv('regularData\\\\comp1.csv')\n",
    "df_comp4 = pd.read_csv('regularData\\\\comp1.csv')\n",
    "df_comp5 = pd.read_csv('regularData\\\\comp1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {}  # A dictionary to hold your new DataFrames\n",
    "\n",
    "for data_type in df_main['Website Data Type'].unique():\n",
    "    # Create a new DataFrame for this data_type\n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    # Add data from the main company\n",
    "    df_main_tmp = df_main[df_main['Website Data Type'] == data_type].copy()\n",
    "    df_main_tmp['Source'] = 'Main'\n",
    "    df_new = pd.concat([df_new, df_main_tmp], ignore_index=True)\n",
    "\n",
    "    # Add data from each competitor\n",
    "    for i, df_comp in enumerate([df_comp1, df_comp2, df_comp3, df_comp4, df_comp5]):\n",
    "        df_comp_tmp = df_comp[df_comp['Website Data Type'] == data_type].copy()\n",
    "        df_comp_tmp['Source'] = f'Competitor {i+1}'\n",
    "        df_new = pd.concat([df_new, df_comp_tmp], ignore_index=True)\n",
    "        print(df_new)\n",
    "        df_new.to_csv('Merged.csv',index=False)\n",
    "\n",
    "    # Store the new DataFrame in the dictionary\n",
    "    dataframes[data_type] = df_new\n",
    "    print(dataframes)\n",
    "\n",
    "    scaler = MinMaxScaler()  # Initialize a scaler\n",
    "\n",
    "# Apply MinMaxScaler to each DataFrame\n",
    "for data_type, df in dataframes.items():\n",
    "    df['Website Data'] = scaler.fit_transform(df[['Website Data']])\n",
    "    dataframes[data_type] = df\n",
    "\n",
    "print(len(dataframes))\n",
    "#dataframes['SERP Features Traffic Branded']['Website Data'][2]=5.0\n",
    "#dataframes['SERP Features Traffic Branded']['Website Data'][3]=4.5\n",
    "#dataframes['SERP Features Traffic Branded']['Website Data'][4]=4.3\n",
    "print(dataframes['Bounce Rate'])\n",
    "print(dataframes['SERP Features Positions Branded'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()  # Initialize a scaler\n",
    "\n",
    "# Apply MinMaxScaler to each DataFrame\n",
    "for data_type, df in dataframes.items():\n",
    "    df['Website Data'] = scaler.fit_transform(df[['Website Data']])\n",
    "    dataframes[data_type] = df\n",
    "\n",
    "print(len(dataframes))\n",
    "print(dataframes['Bounce Rate'])\n",
    "#print(dataframes['SERP Features Traffic Branded'])\n",
    "print(dataframes['Number of Keywords 3-10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Create an empty DataFrame\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "# Concatenate all the DataFrames (main company and competitors)\n",
    "for i, df_comp in enumerate([df_main, df_comp1, df_comp2, df_comp3, df_comp4, df_comp5]):\n",
    "    df_comp_tmp = df_comp.copy()\n",
    "    df_comp_tmp['Source'] = 'Main' if i == 0 else f'Competitor {i}'\n",
    "    df_all = pd.concat([df_all, df_comp_tmp], ignore_index=True)\n",
    "\n",
    "# Convert 'Website Data' to numeric\n",
    "df_all['Website Data'] = pd.to_numeric(df_all['Website Data'], errors='coerce')\n",
    "\n",
    "# Separate numeric and non-numeric data\n",
    "numeric = df_all.select_dtypes(include=[np.number])\n",
    "non_numeric = df_all.select_dtypes(exclude=[np.number])\n",
    "\n",
    "# Apply the imputer\n",
    "imputed = imputer.fit_transform(numeric)\n",
    "\n",
    "# Combine numeric and non-numeric data\n",
    "df_all = pd.concat([non_numeric.reset_index(drop=True),\n",
    "                    pd.DataFrame(imputed, columns=numeric.columns)], axis=1)\n",
    "\n",
    "# Initialize a scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "\n",
    "dataframes = {}  # A dictionary to hold your new DataFrames\n",
    "\n",
    "for data_type in df_main['Website Data Type'].unique():\n",
    "    # Create a new DataFrame for this data_type\n",
    "    df_new = pd.DataFrame()\n",
    "\n",
    "    # Add data from the main company\n",
    "    df_main_tmp = df_main[df_main['Website Data Type'] == data_type].copy()\n",
    "    df_main_tmp['Source'] = 'Main'\n",
    "    df_new = pd.concat([df_new, df_main_tmp], ignore_index=True)\n",
    "\n",
    "    # Add data from each competitor\n",
    "    for i, df_comp in enumerate([df_comp1, df_comp2, df_comp3, df_comp4, df_comp5]):\n",
    "        df_comp_tmp = df_comp[df_comp['Website Data Type'] == data_type].copy()\n",
    "        df_comp_tmp['Source'] = f'Competitor {i+1}'\n",
    "        df_new = pd.concat([df_new, df_comp_tmp], ignore_index=True)\n",
    "\n",
    "    # Store the new DataFrame in the dictionary\n",
    "    dataframes[data_type] = df_new\n",
    "\n",
    "#dataframes['SERP Features Traffic Branded']['Website Data'][2]=5.0\n",
    "#dataframes['SERP Features Traffic Branded']['Website Data'][3]=4.5\n",
    "#dataframes['SERP Features Traffic Branded']['Website Data'][4]=4.0\n",
    "\n",
    "scaler = MinMaxScaler()  # Initialize a scaler\n",
    "\n",
    "# Apply MinMaxScaler to each DataFrame\n",
    "for data_type, df in dataframes.items():\n",
    "    df['Website Data'] = scaler.fit_transform(df[['Website Data']])\n",
    "    dataframes[data_type] = df\n",
    "\n",
    "print(len(dataframes))\n",
    "print(dataframes['Bounce Rate'])\n",
    "print(dataframes['SERP Features Traffic Branded'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrames for each source\n",
    "df_main = pd.DataFrame()\n",
    "df_comp1 = pd.DataFrame()\n",
    "df_comp2 = pd.DataFrame()\n",
    "df_comp3 = pd.DataFrame()\n",
    "df_comp4 = pd.DataFrame()\n",
    "df_comp5 = pd.DataFrame()\n",
    "\n",
    "# Consolidate the data for each source\n",
    "for df in dataframes.values():\n",
    "    df_main = pd.concat([df_main, df[df['Source'] == 'Main']], ignore_index=True)\n",
    "    df_comp1 = pd.concat([df_comp1, df[df['Source'] == 'Competitor 1']], ignore_index=True)\n",
    "    df_comp2 = pd.concat([df_comp2, df[df['Source'] == 'Competitor 2']], ignore_index=True)\n",
    "    df_comp3 = pd.concat([df_comp3, df[df['Source'] == 'Competitor 3']], ignore_index=True)\n",
    "    df_comp4 = pd.concat([df_comp4, df[df['Source'] == 'Competitor 4']], ignore_index=True)\n",
    "    df_comp5 = pd.concat([df_comp5, df[df['Source'] == 'Competitor 5']], ignore_index=True)\n",
    "print(df_comp2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_comp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Write each DataFrame to a CSV file\n",
    "df_main.to_csv('regularData\\\\main.csv', index=False)\n",
    "df_comp1.to_csv('regularData\\\\comp1.csv', index=False)\n",
    "df_comp1.to_csv('regularData\\\\comp2.csv', index=False)\n",
    "df_comp3.to_csv('regularData\\\\comp3.csv', index=False)\n",
    "df_comp4.to_csv('regularData\\\\comp4.csv', index=False)\n",
    "df_comp5.to_csv('regularData\\\\comp5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all data\n",
    "dataframes = {\n",
    "    'Main': pd.read_csv('regularData\\\\main.csv'),\n",
    "    'First Competitor': pd.read_csv('regularData\\\\comp1.csv'),\n",
    "    'Second Competitor': pd.read_csv('regularData\\\\comp2.csv'),\n",
    "    'Third Competitor': pd.read_csv('regularData\\\\comp3.csv'),\n",
    "    'Fourth Competitor': pd.read_csv('regularData\\\\comp4.csv'),\n",
    "    'Fifth Competitor': pd.read_csv('regularData\\\\comp5.csv')\n",
    "}\n",
    "weights_df = pd.read_csv('weights.csv')\n",
    "category_weights_df = pd.read_csv('category_weights.csv')\n",
    "\n",
    "def calculate_scores(df, weights_df, category_weights_df):\n",
    "    # Multiply metrics by their weights\n",
    "    df = df.set_index('Website Data Type')\n",
    "    weights_df = weights_df.set_index('Metric')\n",
    "\n",
    "    # If metric is missing in df, distribute its weight to other metrics in the same category\n",
    "    for metric in weights_df.index:\n",
    "        if metric not in df.index:\n",
    "            missing_weight = weights_df.loc[metric, 'Weight']\n",
    "            category = weights_df.loc[metric, 'Category']\n",
    "            weights_df.loc[weights_df['Category'] == category, 'Weight'] += missing_weight / (weights_df['Category'] == category).sum()\n",
    "            weights_df.loc[metric, 'Weight'] = 0 # Set missing weight to zero\n",
    "\n",
    "    # Update weights in df\n",
    "    df['Weight'] = weights_df['Weight']\n",
    "    weighted_metrics = df['Website Data'].mul(df['Weight'])\n",
    "\n",
    "    # Group by category and sum\n",
    "    category_sums = weighted_metrics.groupby(weights_df['Category']).sum()\n",
    "\n",
    "    # If a category is missing, distribute its weight to other categories\n",
    "    for category in category_weights_df['Category']:\n",
    "        if category not in category_sums.index:\n",
    "            missing_weight = category_weights_df.loc[category_weights_df['Category'] == category, 'Category Weight'].values[0]\n",
    "            category_weights_df['Category Weight'] += missing_weight / len(category_weights_df)\n",
    "            category_weights_df.loc[category_weights_df['Category'] == category, 'Category Weight'] = 0 # Set missing weight to zero\n",
    "\n",
    "    # Multiply category sums by their weights\n",
    "    weighted_categories = category_sums.mul(category_weights_df.set_index('Category')['Category Weight'])\n",
    "\n",
    "    return weighted_categories\n",
    "\n",
    "# Calculate category scores and overall score for each competitor\n",
    "final_ratings = {}\n",
    "for competitor, df in dataframes.items():\n",
    "    try:\n",
    "        if competitor != 'Main':\n",
    "            scores = calculate_scores(df, weights_df, category_weights_df)\n",
    "            final_ratings[competitor] = scores.sum()\n",
    "    except:\n",
    "        final_ratings[competitor] = 0\n",
    "\n",
    "# Calculate weighted average of competitors' scores\n",
    "competitors_weights = weights_df[weights_df['Category'] == 'Competitor Analysis']\n",
    "competitors_weighted_scores = 0\n",
    "\n",
    "for _, row in competitors_weights.iterrows():\n",
    "    competitors_weighted_scores += row['Weight'] * final_ratings.get(row['Metric'], 0) # Use get() to prevent key error\n",
    "\n",
    "new_row = pd.DataFrame({'Website Data Type': ['Competitor Analysis'], 'Website Data': [competitors_weighted_scores], 'Source': ['Main']})\n",
    "# Add 'Competitors Score' to 'Main' dataframe\n",
    "dataframes['Main'] = pd.concat([dataframes['Main'], new_row], ignore_index=True)\n",
    "\n",
    "# Calculate category scores and overall score for main company\n",
    "try:\n",
    "    main_scores = calculate_scores(dataframes['Main'], weights_df, category_weights_df)\n",
    "    main_final_score = main_scores.sum()\n",
    "except:\n",
    "    main_final_score = 0\n",
    "\n",
    "print(\"Final Ratings for Competitors:\")\n",
    "for competitor, rating in final_ratings.items():\n",
    "    print(f\"{competitor}: {rating}\")\n",
    "print(\"\\nFinal Score for Main Company:\", main_final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all data\n",
    "dataframes = {\n",
    "    'Main': pd.read_csv('regularData\\\\main.csv'),\n",
    "    'First Competitor': pd.read_csv('regularData\\\\comp1.csv'),\n",
    "    'Second Competitor': pd.read_csv('regularData\\\\comp2.csv'),\n",
    "    'Third Competitor': pd.read_csv('regularData\\\\comp3.csv'),\n",
    "    'Fourth Competitor': pd.read_csv('regularData\\\\comp4.csv'),\n",
    "    'Fifth Competitor': pd.read_csv('regularData\\\\comp5.csv')\n",
    "}\n",
    "\n",
    "weights_df = pd.read_csv('weights.csv')\n",
    "category_weights_df = pd.read_csv('category_weights.csv')\n",
    "\n",
    "def distribute_nan_weights(df, weights_df):\n",
    "    # Find missing values\n",
    "    nan_metrics = df['Website Data Type'][df['Website Data'].isna()]\n",
    "\n",
    "    for metric in nan_metrics:\n",
    "        if metric not in weights_df['Metric'].values:\n",
    "            print(f\"Metric '{metric}' not found in weights DataFrame\")\n",
    "            continue\n",
    "\n",
    "        # Get category of the metric\n",
    "        category = weights_df.loc[weights_df['Metric'] == metric, 'Category'].values[0]\n",
    "\n",
    "        # Get total weight of the category\n",
    "        total_weight = weights_df.loc[weights_df['Category'] == category, 'Weight'].sum()\n",
    "\n",
    "        # Get weight of the metric with missing value\n",
    "        nan_weight = weights_df.loc[weights_df['Metric'] == metric, 'Weight'].values[0]\n",
    "\n",
    "        # Distribute the weight of the metric with missing value\n",
    "        weights_df.loc[weights_df['Category'] == category, 'Weight'] += nan_weight / (total_weight - nan_weight)\n",
    "\n",
    "        # Set the weight of the metric with missing value to 0\n",
    "        weights_df.loc[weights_df['Metric'] == metric, 'Weight'] = 0\n",
    "\n",
    "def calculate_scores(df, weights_df, category_weights_df):\n",
    "    try:\n",
    "        # Distribute weights of NaN values\n",
    "        distribute_nan_weights(df, weights_df)\n",
    "\n",
    "        # Multiply metrics by their weights\n",
    "        weighted_metrics = df.set_index('Website Data Type')['Website Data'].mul(weights_df.set_index('Metric')['Weight'])\n",
    "\n",
    "        # Group by category and sum\n",
    "        category_sums = weighted_metrics.groupby(weights_df.set_index('Metric')['Category']).sum()\n",
    "\n",
    "        # Multiply category sums by their weights\n",
    "        weighted_categories = category_sums.mul(category_weights_df.set_index('Category')['Category Weight'])\n",
    "\n",
    "        return weighted_categories\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.Series([0])  # Return zero if there's an error\n",
    "\n",
    "# Calculate category scores and overall score for each competitor\n",
    "final_ratings = {}\n",
    "for competitor, df in dataframes.items():\n",
    "    if competitor != 'Main':\n",
    "        scores = calculate_scores(df, weights_df, category_weights_df)\n",
    "        if isinstance(scores, pd.Series):\n",
    "            final_ratings[competitor] = scores.sum()\n",
    "\n",
    "# Calculate weighted average of competitors' scores\n",
    "competitors_weights = weights_df[weights_df['Category'] == 'Competitor Analysis']\n",
    "competitors_weighted_scores = 0\n",
    "\n",
    "for _, row in competitors_weights.iterrows():\n",
    "    competitors_weighted_scores += row['Weight'] * final_ratings.get(row['Metric'], 0)\n",
    "\n",
    "new_row = pd.DataFrame({'Website Data Type': ['Competitor Analysis'], 'Website Data': [competitors_weighted_scores], 'Source': ['Main']})\n",
    "# Add 'Competitors Score' to 'Main' dataframe\n",
    "dataframes['Main'] = pd.concat([dataframes['Main'], new_row], ignore_index=True)\n",
    "\n",
    "# Calculate category scores and overall score for main company\n",
    "main_scores = calculate_scores(dataframes['Main'], weights_df, category_weights_df)\n",
    "main_final_score = main_scores.sum() if isinstance(main_scores, pd.Series) else 0\n",
    "\n",
    "print(\"Final Ratings for Competitors:\")\n",
    "for competitor, rating in final_ratings.items():\n",
    "    print(f\"{competitor}: {rating}\")\n",
    "print(\"\\nFinal Score for Main Company:\", main_final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all data\n",
    "dataframes = {\n",
    "    'Main': pd.read_csv('regularData\\\\main.csv'),\n",
    "    'First Competitor': pd.read_csv('regularData\\\\comp1.csv'),\n",
    "    'Second Competitor': pd.read_csv('regularData\\\\comp2.csv'),\n",
    "    'Third Competitor': pd.read_csv('regularData\\\\comp3.csv'),\n",
    "    'Fourth Competitor': pd.read_csv('regularData\\\\comp4.csv'),\n",
    "    'Fifth Competitor': pd.read_csv('regularData\\\\comp5.csv')\n",
    "}\n",
    "\n",
    "weights_df = pd.read_csv('weights.csv')\n",
    "category_weights_df = pd.read_csv('category_weights.csv')\n",
    "\n",
    "def distribute_nan_weights(df, weights_df):\n",
    "    # Find missing values\n",
    "    nan_metrics = df['Website Data Type'][df['Website Data'].isna()]\n",
    "\n",
    "    for metric in nan_metrics:\n",
    "        if metric not in weights_df['Metric'].values:\n",
    "            print(f\"Metric '{metric}' not found in weights DataFrame\")\n",
    "            continue\n",
    "\n",
    "        # Get category of the metric\n",
    "        category = weights_df.loc[weights_df['Metric'] == metric, 'Category'].values[0]\n",
    "\n",
    "        # Get total weight of the category\n",
    "        total_weight = weights_df.loc[weights_df['Category'] == category, 'Weight'].sum()\n",
    "\n",
    "        # Get weight of the metric with missing value\n",
    "        nan_weight = weights_df.loc[weights_df['Metric'] == metric, 'Weight'].values[0]\n",
    "\n",
    "        # Distribute the weight of the metric with missing value\n",
    "        weights_df.loc[weights_df['Category'] == category, 'Weight'] += nan_weight / (total_weight - nan_weight)\n",
    "\n",
    "        # Set the weight of the metric with missing value to 0\n",
    "        weights_df.loc[weights_df['Metric'] == metric, 'Weight'] = 0\n",
    "\n",
    "def calculate_scores(df, weights_df, category_weights_df):\n",
    "    try:\n",
    "        # Distribute weights of NaN values\n",
    "        distribute_nan_weights(df, weights_df)\n",
    "\n",
    "        # Multiply metrics by their weights\n",
    "        weighted_metrics = df.set_index('Website Data Type')['Website Data'].mul(weights_df.set_index('Metric')['Weight'])\n",
    "        print(f\"Weighted metrics: {weighted_metrics}\")\n",
    "\n",
    "        # Group by category and sum\n",
    "        category_sums = weighted_metrics.groupby(weights_df.set_index('Metric')['Category']).sum()\n",
    "        print(f\"Category sums: {category_sums}\")\n",
    "\n",
    "        # Multiply category sums by their weights\n",
    "        weighted_categories = category_sums.mul(category_weights_df.set_index('Category')['Category Weight'])\n",
    "        print(f\"Weighted categories: {weighted_categories}\")\n",
    "\n",
    "        return weighted_categories\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.Series([0])  # Return zero if there's an error\n",
    "\n",
    "# Calculate category scores and overall score for each competitor\n",
    "final_ratings = {}\n",
    "for competitor, df in dataframes.items():\n",
    "    if competitor != 'Main':\n",
    "        print(f\"Calculating scores for {competitor}...\")\n",
    "        scores = calculate_scores(df, weights_df, category_weights_df)\n",
    "        if isinstance(scores, pd.Series):\n",
    "            print(f\"Scores for {competitor}: {scores}\")\n",
    "            final_ratings[competitor] = scores.sum()\n",
    "\n",
    "# Calculate weighted average of competitors' scores\n",
    "competitors_weights = weights_df[weights_df['Category'] == 'Competitor Analysis']\n",
    "competitors_weighted_scores = 0\n",
    "\n",
    "for _, row in competitors_weights.iterrows():\n",
    "    competitors_weighted_scores += row['Weight'] * final_ratings.get(row['Metric'], 0)\n",
    "\n",
    "new_row = pd.DataFrame({'Website Data Type': ['Competitor Analysis'], 'Website Data': [competitors_weighted_scores], 'Source': ['Main']})\n",
    "# Add 'Competitors Score' to 'Main' dataframe\n",
    "dataframes['Main'] = pd.concat([dataframes['Main'], new_row], ignore_index=True)\n",
    "\n",
    "# Calculate category scores and overall score for main company\n",
    "main_scores = calculate_scores(dataframes['Main'], weights_df, category_weights_df)\n",
    "main_final_score = main_scores.sum() if isinstance(main_scores, pd.Series) else 0\n",
    "\n",
    "print(\"Final Ratings for Competitors:\")\n",
    "for competitor, rating in final_ratings.items():\n",
    "    print(f\"{competitor}: {rating}\")\n",
    "print(\"\\nFinal Score for Main Company:\", main_final_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load main and competitors data\n",
    "dataframes = {\n",
    "    'Main': pd.read_csv('regularData\\\\main.csv'),\n",
    "    'First Competitor': pd.read_csv('regularData\\\\comp1.csv'),\n",
    "    'Second Competitor': pd.read_csv('regularData\\\\comp2.csv'),\n",
    "    'Third Competitor': pd.read_csv('regularData\\\\comp3.csv'),\n",
    "    'Fourth Competitor': pd.read_csv('regularData\\\\comp4.csv'),\n",
    "    'Fifth Competitor': pd.read_csv('regularData\\\\comp5.csv')\n",
    "}\n",
    "print(dataframes['Main'])\n",
    "\n",
    "# Load social media data for main and competitors\n",
    "social_dataframes = {\n",
    "    'Main': pd.read_csv('regularDataSocial\\\\processed_social_media_Main.csv'),\n",
    "    'First Competitor': pd.read_csv('regularDataSocial\\\\processed_social_media_Competior_1.csv'),\n",
    "    'Second Competitor': pd.read_csv('regularDataSocial\\\\processed_social_media_Competior_2.csv'),\n",
    "    'Third Competitor': pd.read_csv('regularDataSocial\\\\processed_social_media_Competior_3.csv'),\n",
    "    'Fourth Competitor': pd.read_csv('regularDataSocial\\\\processed_social_media_Competior_4.csv'),\n",
    "    'Fifth Competitor': pd.read_csv('regularDataSocial\\\\processed_social_media_Competior_5.csv')\n",
    "}\n",
    "print(social_dataframes['Main'])\n",
    "\n",
    "# Ensure platform names are all in the same case\n",
    "weights_df = pd.read_csv('weights.csv')\n",
    "weights_df['Platform'] = weights_df['Platform'].str.capitalize()\n",
    "category_weights_df = pd.read_csv('category_weights.csv')\n",
    "platform_weights = pd.read_csv('platform_weights.csv')\n",
    "platform_weights['Platform'] = platform_weights['Platform'].str.capitalize()\n",
    "print(platform_weights.columns)\n",
    "print(platform_weights[platform_weights.duplicated(['Platform'])])\n",
    "\n",
    "\n",
    "for company, social_media_df in social_dataframes.items():\n",
    "    social_media_df['Key'] = social_media_df['Social Data Type'] + social_media_df['Social Platform']\n",
    "    print(social_media_df[social_media_df.duplicated(['Key'])])\n",
    "\n",
    "\n",
    "# Also ensure the platform names in social media data are in the same case\n",
    "for company, df in social_dataframes.items():\n",
    "    df['Social Platform'] = df['Social Platform'].str.capitalize()\n",
    "\n",
    "def distribute_nan_weights(df, weights_df):\n",
    "    # Find missing values\n",
    "    nan_metrics = df['Website Data Type'][df['Website Data'].isna()]\n",
    "\n",
    "    for metric in nan_metrics:\n",
    "        if metric not in weights_df['Metric'].values:\n",
    "            print(f\"Metric '{metric}' not found in weights DataFrame\")\n",
    "            continue\n",
    "\n",
    "        # Get category of the metric\n",
    "        category = weights_df.loc[weights_df['Metric'] == metric, 'Category'].values[0]\n",
    "\n",
    "        # Get total weight of the category\n",
    "        total_weight = weights_df.loc[weights_df['Category'] == category, 'Weight'].sum()\n",
    "\n",
    "        # Get weight of the metric with missing value\n",
    "        nan_weight = weights_df.loc[weights_df['Metric'] == metric, 'Weight'].values[0]\n",
    "\n",
    "        # Distribute the weight of the metric with missing value\n",
    "        weights_df.loc[weights_df['Category'] == category, 'Weight'] += nan_weight / (total_weight - nan_weight)\n",
    "\n",
    "        # Set the weight of the metric with missing value to 0\n",
    "        weights_df.loc[weights_df['Metric'] == metric, 'Weight'] = 0\n",
    "\n",
    "def calculate_scores(df, weights_df, category_weights_df):\n",
    "    try:\n",
    "        # Distribute weights of NaN values\n",
    "        distribute_nan_weights(df, weights_df)\n",
    "        \n",
    "        # Multiply metrics by their weights\n",
    "        weighted_metrics = df.set_index('Website Data Type')['Website Data'].mul(weights_df.set_index('Metric')['Weight'])\n",
    "\n",
    "        # Group by category and sum\n",
    "        category_sums = weighted_metrics.groupby(weights_df.set_index('Metric')['Category']).sum()\n",
    "\n",
    "        # Multiply category sums by their weights\n",
    "        weighted_categories = category_sums.mul(category_weights_df.set_index('Category')['Category Weight'])\n",
    "\n",
    "        return weighted_categories\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.Series([0])  # Return zero if there's an error\n",
    "\n",
    "def calculate_social_media_score(social_media_data, platform_weights):\n",
    "    # Prepare data\n",
    "    social_media_data['Key'] = social_media_data['Social Data Type'] + social_media_data['Social Platform']\n",
    "    platform_weights['Key'] = platform_weights['Metric'] + platform_weights['Platform']\n",
    "    platform_weights.set_index('Key', inplace=True)\n",
    "\n",
    "    # Multiply metrics by their weights\n",
    "    weighted_metrics = social_media_data['Social Data'].mul(platform_weights.loc[social_media_data['Key'], 'Weight'])\n",
    "\n",
    "    # Sum all metrics\n",
    "    social_media_score = weighted_metrics.sum()\n",
    "\n",
    "    return social_media_score\n",
    "\n",
    "\n",
    "\n",
    "# Calculate social media score and add it to dataframes\n",
    "for company, social_media_df in social_dataframes.items():\n",
    "    social_media_score = calculate_social_media_score(social_media_df, weights_df)\n",
    "    new_row = pd.DataFrame({'Website Data Type': ['Social Media Performance'], 'Website Data': [social_media_score], 'Source': [company]})\n",
    "    dataframes[company] = pd.concat([dataframes[company], new_row], ignore_index=True)\n",
    "\n",
    "\n",
    "# Calculate category scores and overall score for each competitor\n",
    "final_ratings = {}\n",
    "for competitor, df in dataframes.items():\n",
    "    if competitor != 'Main':\n",
    "        print(f\"Calculating scores for {competitor}...\")\n",
    "        scores = calculate_scores(df, weights_df, category_weights_df)\n",
    "        if isinstance(scores, pd.Series):\n",
    "            print(f\"Scores for {competitor}: {scores}\")\n",
    "            final_ratings[competitor] = scores.sum()\n",
    "\n",
    "# Calculate weighted average of competitors' scores\n",
    "competitors_weights = weights_df[weights_df['Category'] == 'Competitor Analysis']\n",
    "competitors_weighted_scores = 0\n",
    "\n",
    "for _, row in competitors_weights.iterrows():\n",
    "    competitors_weighted_scores += row['Weight'] * final_ratings.get(row['Metric'], 0)\n",
    "\n",
    "new_row = pd.DataFrame({'Website Data Type': ['Competitor Analysis'], 'Website Data': [competitors_weighted_scores], 'Source': ['Main']})\n",
    "# Add 'Competitors Score' to 'Main' dataframe\n",
    "dataframes['Main'] = pd.concat([dataframes['Main'], new_row], ignore_index=True)\n",
    "\n",
    "# Calculate category scores and overall score for main company\n",
    "main_scores = calculate_scores(dataframes['Main'], weights_df, category_weights_df)\n",
    "main_final_score = main_scores.sum() if isinstance(main_scores, pd.Series) else 0\n",
    "\n",
    "print(\"Final Ratings for Competitors:\")\n",
    "for competitor, rating in final_ratings.items():\n",
    "    print(f\"{competitor}: {rating}\")\n",
    "print(\"\\nFinal Score for Main Company:\", main_final_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
